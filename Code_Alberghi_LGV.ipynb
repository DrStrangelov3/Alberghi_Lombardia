{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lodging structures classifier in Lombardy, Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import re\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_0 = pd.read_csv('classification_dataset.csv', sep='\\t') \n",
    "data = data_0\n",
    "\n",
    "extra = pd.read_csv('cusersplenevicidocumentsdocs-lavorockancvsxr2pacomuniitaliani24102017 (1).csv', sep=None,  engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ID\n",
    "data.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LOCALITA\n",
    "data['LOCALITA'] = data['LOCALITA'].apply(lambda x: 0 if pd.isnull(x) else 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#IN ABITATO\n",
    "data['IN_ABITATO' ].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SUL_LAGO\n",
    "data['SUL_LAGO'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#VICINO_ELIPORTO\n",
    "data.drop(['VICINO_ELIPORTO'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#VICINO_AEREOPORTO\n",
    "data['VICINO_AEREOPORTO'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#VICINO_IMP_RISALITA\n",
    "data['VICINO_IMP_RISALITA'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ZONA_CENTRALE\n",
    "data['ZONA_CENTRALE'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ZONA_PERIFERICA\n",
    "data['ZONA_PERIFERICA'].fillna(0, inplace=True)\n",
    "\n",
    "#place both in ZONA_PERIFERICA and ZONA_CENTRALE =1 set to 0 on both\n",
    "data[['ZONA_PERIFERICA', 'ZONA_CENTRALE']].loc[data['ZONA_PERIFERICA'] == 1].loc[data['ZONA_CENTRALE'] ==1 ]\n",
    "data['ZONA_CENTRALE'].iloc[192] = 0\n",
    "data['ZONA_PERIFERICA'].iloc[192] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ZONA_STAZIONE_FS\n",
    "data['ZONA_STAZIONE_FS'] = data['ZONA_STAZIONE_FS'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ATTREZATURE_VARIE\n",
    "data['ATTREZZATURE_VARIE'] = data['ATTREZZATURE_VARIE'].fillna('').apply(lambda x: x.split(',')) #fills nan with '' and split the strings\n",
    "dummies_a = pd.get_dummies(data['ATTREZZATURE_VARIE'].apply(pd.Series).stack()).sum(level=0) #extracts dummies from lists of strings\n",
    "dummies_a = dummies_a.replace(2,1) #correction for repeated in the strings \n",
    "dummies_a = dummies_a[dummies_a.columns[dummies_a.sum() >= 75][1:]] #select only attrezature that appears >= 75 and drops the '' \n",
    "data = pd.concat([data, dummies_a], axis=1, join_axes=[data.index]) #joins dummies to the original df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CARTE_ACCETTATE\n",
    "data[['CARTE_ACCETTATE']] = data[['CARTE_ACCETTATE']].apply(lambda x: 0 if pd.isnull(x[0]) else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LINGUE_PARLATE\n",
    "#counting how many different languages are spoken\n",
    "data['LINGUE_PARLATE'] = data['LINGUE_PARLATE'].fillna('-').apply(lambda x: x.split(','))\n",
    "data['LINGUE_PARLATE'] = data['LINGUE_PARLATE'].apply(lambda x: len(x) if x[0] != '-' else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SPORT\n",
    "data['SPORT'] = data['SPORT'].fillna('').apply(lambda x: x.split(',')) #fills nan with '' and split the strings\n",
    "dummies_s = pd.get_dummies(data['SPORT'].apply(pd.Series).stack()).sum(level=0) \n",
    "dummies_a = dummies_a.replace(2,1)\n",
    "\n",
    "#aggregate all sports with a field\n",
    "sport_con_campo = [' calcetto',' pallacanestro','Campo attrezzato per pallavolo',\n",
    "'Campo da calcetto', 'Campo da calcio','Campo da pallavolo', 'Campo da tennis', 'Campo da bocce']\n",
    "dummies_s['Sport_Con_Campo'] = dummies_s[sport_con_campo].sum(axis=1)\n",
    "\n",
    "#aggregate all summer sports\n",
    "sport_estivi = [' beach volley', 'Sci nautico','Sub','Vela','Windsurf']\n",
    "dummies_s['Sport_Estivi'] = dummies_s[sport_estivi] .sum(axis=1)\n",
    "\n",
    "#aggregate all winter sports\n",
    "sport_invernali = ['Pattinaggio su ghiaccio', 'Sci da fondo', 'Sci montano', 'Sport invernali']\n",
    "dummies_s['Sport_Invernali'] = dummies_s[sport_invernali].sum(axis=1)\n",
    "\n",
    "#aggregate swimming pool\n",
    "dummies_s['Piscina'] = dummies_s[['Piscina coperta','Piscina scoperta']].sum(axis=1)\n",
    "\n",
    "#aggregate all indoor activiteis\n",
    "sport_indoor = ['Bigliardo','Bowling','Calciobalilla', 'Tennis da tavolo']\n",
    "dummies_s['Sport_Indoor'] = dummies_s[sport_indoor].sum(axis=1)\n",
    "\n",
    "#there is 'Fitness/centro salute' also as a ATTREZATURA. I'm combinig it with the sport column\n",
    "Fitness_centro_salute = pd.concat([dummies_s['Fitness/centro salute'], dummies_a['Fitness/centro salute']], axis=1, join_axes=[data.index])\n",
    "data['Fitness/centro salute']  = Fitness_centro_salute.sum(axis=1).replace(2,1)\n",
    "\n",
    "#dummify our aggregation of SPORT and combine it with our dataset\n",
    "dummies_s = dummies_s[['Sport_Con_Campo','Sport_Estivi','Sport_Invernali','Piscina']]\n",
    "dummies_s[dummies_s != 0] = 1 #replace values > 1 with 1 given that we summed the columns\n",
    "data = pd.concat([data, dummies_s], axis=1, join_axes=[data.index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CONGRESSI\n",
    "data['CONGRESSI'] = data['CONGRESSI'].fillna('0').apply(lambda x: re.search(r'(\\d+){1}', x)[0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#OUTPUT\n",
    "#convert classes to numbers\n",
    "output = {'Case_Appartamenti':1, 'Campeggio':2, 'B&B':3, '1_a_3_Stelle':4, '4_a_5_Stelle':5 }\n",
    "data[['OUTPUT']] = data['OUTPUT'].map(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for Lombardia\n",
    "extra = extra.loc[extra['Regione'] == 'Lombardia']\n",
    "\n",
    "#drop unwanted columns\n",
    "extra = extra.drop(['ISTAT', 'Regione', 'PopStraniera', 'AltezzaMinima', 'AltezzaMassima', 'ZonaSismica', 'ClasseComune', \\\n",
    "                    'Latitudine', 'Longitudine', 'ZonaAltimetrica', 'SuperficieKmq', 'Provincia',\\\n",
    "                   'AreaGeo', 'SiglaProv','IndiceMontanita', 'PopResidente'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert ZonaClimatica to bool F==1 the rest = 0\n",
    "extra['ZonaClimatica'] = extra['ZonaClimatica'].apply(lambda x: 1 if x == 'F' else 0)\n",
    "\n",
    "#convert to float\n",
    "extra['DensitaDemografica'] = extra['DensitaDemografica'].str.replace(',','.').astype(float)\n",
    "extra['AltezzaCentro'] = extra['AltezzaCentro'].astype(int)\n",
    "\n",
    "#create bool if comune is also capoluogo \n",
    "extra['TipoComune'] = extra['TipoComune'].apply(lambda x: 0 if x == 'No capoluogo' else 1)\n",
    "extra.rename(columns = {'TipoComune': 'Capoluogo'},inplace=True)\n",
    "\n",
    "#convert GradoUrbaniz to numeric\n",
    "\n",
    "urban_dic = {\n",
    "    'Elevato': 3,\n",
    "    'Medio': 2,\n",
    "    'Basso':1}\n",
    "\n",
    "extra['GradoUrbaniz'] = extra['GradoUrbaniz'].map(urban_dic)\n",
    "\n",
    "#fixing names of comuni that are different between extra and data\n",
    "\n",
    "extra['﻿Comune'] = extra['﻿Comune'].str.upper()\n",
    "\n",
    "nomi_comuni = {\n",
    "    'TOSCOLANO-MADERNO': 'TOSCOLANO MADERNO',\n",
    "    'TREMOSINE SUL GARDA': 'TREMOSINE',\n",
    "    'SALÒ': \"SALO'\",\n",
    "    'LONATO DEL GARDA': 'LONATO',\n",
    "    'PUEGNAGO DEL GARDA': 'PUEGNAGO SUL GARDA',\n",
    "    'TEMÙ':\"TEMU'\",\n",
    "    'CANTÙ': \"CANTU'\",\n",
    "    'ROÈ VOLCIANO': \"ROE' VOLCIANO\",\n",
    "    'GODIASCO SALICE TERME': 'GODIASCO',\n",
    "    'RODENGO SAIANO': 'RODENGO-SAIANO',\n",
    "    'UGGIATE-TREVANO': 'UGGIATE - TREVANO',\n",
    "    'BARZANÒ': \"BARZANO'\",\n",
    "    'GAMBOLÒ': \"GAMBOLO'\",\n",
    "    'MUGGIÒ': 'MUGGI�',\n",
    "    'GADESCO-PIEVE DELMONA': 'GADESCO - PIEVE DELMONA',\n",
    "    'TRAVACÒ SICCOMARIO': \"TRAVACO' SICCOMARIO\",\n",
    "    'VIGGIÙ':\"VIGGIU'\",\n",
    "    'GORNATE-OLONA': 'GORNATE OLONA',\n",
    "    'ALMÈ': \"ALME'\",\n",
    "    \"VILLA D'ALMÈ\": \"VILLA D'ALME'\",\n",
    "    \"CASSINA DE' PECCHI\" : \"CASSINA DE PECCHI \"}\n",
    "    \n",
    "extra['﻿Comune'].replace(nomi_comuni, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "extra.rename(columns = {'﻿Comune': 'Comune'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#union with data and extra\n",
    "union = pd.merge(data, extra, how='left', left_on= data['COMUNE'], right_on= extra['Comune'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop non numerical columns\n",
    "\n",
    "union = union.drop(['PROVINCIA', 'COMUNE', 'LATITUDINE', 'LONGITUDINE','ATTREZZATURE_VARIE','SPORT', 'Comune'], axis=1)\n",
    "union.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final dataset, now commented to no be re-run\n",
    "#union.to_csv('union.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = union['OUTPUT']\n",
    "X = union.drop(['OUTPUT'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = MinMaxScaler()\n",
    "X_scaled = normal.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA()\n",
    "# X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "rfe = RFECV(SVC(kernel='linear'), cv=5, scoring='accuracy', verbose=0, n_jobs=-1) #if stuck restart kernel\n",
    "rfe.fit(X_train, Y_train)\n",
    "X_train = rfe.transform(X_train)\n",
    "X_test = rfe.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.6s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8779767762251526\n",
      "{'rf__class_weight': 'balanced_subsample', 'rf__criterion': 'gini', 'rf__min_samples_split': 5, 'rf__n_estimators': 77}\n",
      "0.8949232585596222\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "pipe_rf = Pipeline([\n",
    "    ('rf', rf)\n",
    " ])\n",
    "\n",
    "\n",
    "params_rf = {\n",
    "    'rf__criterion': ['gini'],\n",
    "    'rf__n_estimators': [77],\n",
    "    'rf__min_samples_split': [5,6],\n",
    "    'rf__class_weight': [\"balanced_subsample\"]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "gs_rf = GridSearchCV(pipe_rf, param_grid= params_rf, n_jobs=-1 , verbose=1, cv=5)\n",
    "\n",
    "gs_rf.fit(X_train,Y_train)\n",
    "print(gs_rf.best_score_)\n",
    "print(gs_rf.best_params_)\n",
    "preds = gs_rf.predict(X_test)\n",
    "print(accuracy_score(preds, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix in a dataframe\n",
    "confusion_matrix_RF = pd.DataFrame(confusion_matrix(preds, Y_test),columns= \\\n",
    "             ['Case_Appartamenti','Campeggio','B&B','1_a_3_Stelle','4_a_5_Stelle'], \\\n",
    "             index=['Case_Appartamenti','Campeggio','B&B','1_a_3_Stelle','4_a_5_Stelle'])\n",
    "\n",
    "confusion_matrix_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|                   |   Case_Appartamenti |   Campeggio |   B&B |   1_a_3_Stelle |   4_a_5_Stelle |\n",
    "|:------------------|--------------------:|------------:|------:|---------------:|---------------:|\n",
    "| Case_Appartamenti |                 350 |          29 |     2 |              0 |              7 |\n",
    "| Campeggio         |                  27 |          69 |     0 |              0 |              0 |\n",
    "| B&B               |                   2 |           0 |   392 |              0 |             43 |\n",
    "| 1_a_3_Stelle      |                   0 |           0 |     0 |             26 |              0 |\n",
    "| 4_a_5_Stelle      |                  14 |           6 |    47 |              1 |            679 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   13.4s remaining:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8852588073213934\n",
      "{'xgb__booster': 'gbtree', 'xgb__gamma': 0, 'xgb__learning_rate': 0.1, 'xgb__max_depth': 5, 'xgb__n_estimators': 160, 'xgb__reg_alpha': 0, 'xgb__reg_lambda': 0}\n",
      "0.911452184179457\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "pipe_xgb = Pipeline([\n",
    "    ('xgb',xgb)\n",
    " ])\n",
    "\n",
    "params_xgb = {\n",
    "    'xgb__booster': ['gbtree'],\n",
    "    'xgb__n_estimators': [160], \n",
    "    'xgb__max_depth': [5],\n",
    "    'xgb__reg_alpha': [0],\n",
    "    'xgb__reg_lambda':[0],\n",
    "    'xgb__learning_rate': [0.1],\n",
    "    'xgb__gamma': [0]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "gs_xgb = GridSearchCV(pipe_xgb, param_grid= params_xgb, n_jobs=-1 , verbose=-1, cv=5)\n",
    "\n",
    "gs_xgb.fit(X_train,Y_train)\n",
    "print(gs_xgb.best_score_)\n",
    "print(gs_xgb.best_params_)\n",
    "xgb_preds = gs_xgb.predict(X_test)\n",
    "print(accuracy_score(xgb_preds , Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#builds a confusion matrix in a dataframe\n",
    "confusion_matrix_XGB= pd.DataFrame(confusion_matrix(xgb_preds, Y_test),columns= \\\n",
    "             ['Case_Appartamenti','Campeggio','B&B','1_a_3_Stelle','4_a_5_Stelle'], \\\n",
    "             index=['Case_Appartamenti','Campeggio','B&B','1_a_3_Stelle','4_a_5_Stelle'])\n",
    "\n",
    "#confusion_matrix_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|                   |   Case_Appartamenti |   Campeggio |   B&B |   1_a_3_Stelle |   4_a_5_Stelle |\n",
    "|:------------------|--------------------:|------------:|------:|---------------:|---------------:|\n",
    "| Case_Appartamenti |                 705 |           0 |    60 |              8 |              4 |\n",
    "| Campeggio         |                   0 |          25 |     0 |              0 |              0 |\n",
    "| B&B               |                  21 |           0 |   380 |              1 |              0 |\n",
    "| 1_a_3_Stelle      |                   3 |           2 |     1 |            362 |             28 |\n",
    "| 4_a_5_Stelle      |                   0 |           0 |     0 |             22 |             72 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
